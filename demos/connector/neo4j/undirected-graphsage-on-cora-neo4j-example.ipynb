{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undirected GraphSAGE on the CORA citation network using StellarGraph and Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.connector.neo4j import Neo4JGraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CORA data from Neo4J\n",
    "\n",
    "It is assumed that the cora dataset has already been loaded into Neo4J. This notebook demonstrates how to load cora dataset into Neo4J [here](./load-cora-into-neo4j.ipynb).\n",
    "\n",
    "It is still required to load the node features into memory. We use ```py2neo```, which provides tools to connect to Neo4J databases from Python applications. ```py2neo``` documentation could be found [here](https://py2neo.org/v4/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py2neo\n",
    "\n",
    "# connect to the graph database\n",
    "neo4j_graphdb = py2neo.Graph(\"http://localhost:7474\", username=\"neo4j\", password=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.09 s: Loaded node data from neo4j database to memory\n"
     ]
    }
   ],
   "source": [
    "def get_node_data_from_neo4j(neo4j_graphdb):\n",
    "    fetch_node_query = \"MATCH (node) RETURN id(node), properties(node)\"\n",
    "    # run the query\n",
    "    node_records = neo4j_graphdb.run(fetch_node_query)\n",
    "    # convert the node records into pandas dataframe\n",
    "    return pd.DataFrame(node_records).rename(columns={0: \"id\", 1: \"attr\"})\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "node_data = get_node_data_from_neo4j(neo4j_graphdb)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.2f} s: Loaded node data from neo4j database to memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             features                 subject\n",
       "id                                                                           \n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         Neural_Networks\n",
       "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...           Rule_Learning\n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning\n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  Reinforcement_Learning\n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   Probabilistic_Methods"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data = node_data.set_index(\"id\")\n",
    "attribute_df = node_data[\"attr\"].apply(pd.Series)\n",
    "node_data = attribute_df.drop(labels=[\"ID\"], axis=1)\n",
    "node_features = pd.DataFrame(node_data[\"features\"].values.tolist(), index=node_data.index)\n",
    "node_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to train a graph-ML model that will predict the \"subject\" attribute on the nodes. These subjects are one of 7 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case_Based',\n",
       " 'Genetic_Algorithms',\n",
       " 'Neural_Networks',\n",
       " 'Probabilistic_Methods',\n",
       " 'Reinforcement_Learning',\n",
       " 'Rule_Learning',\n",
       " 'Theory'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(node_data[\"subject\"])\n",
    "set(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning we want to take a subset of the nodes for training, and use the rest for testing. We'll use scikit-learn again to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(\n",
    "    node_data, train_size=0.1, test_size=None, stratify=node_data[\"subject\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to numeric arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our categorical target, we will use one-hot vectors that will be fed into a soft-max Keras layer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = feature_extraction.DictVectorizer(sparse=False)\n",
    "\n",
    "train_targets = target_encoding.fit_transform(train_data[[\"subject\"]].to_dict(\"records\"))\n",
    "test_targets = target_encoding.transform(test_data[[\"subject\"]].to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the GraphSAGE model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a StellarGraph object storing node data. Since the subgraph is sampled directly from Neo4J, we only need to retain the node features and do not need to store any edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarGraph(nodes={\"paper\": node_features}, edges={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stellargraph.core.graph.StellarGraph'>: Undirected multigraph\n",
      " Nodes: 2708, Edges: 0\n",
      "\n",
      " Node types:\n",
      "  paper: [2708]\n",
      "    Edge types: \n",
      "\n",
      " Edge types:\n"
     ]
    }
   ],
   "source": [
    "print(G.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed data from the graph to the Keras model we need a data generator that feeds data from the graph to the model. The generators are specialized to the model and the learning task so we choose the `Neo4JGraphSAGENodeGenerator` as we are predicting node attributes with a GraphSAGE model, sampling directly from Neo4J database.\n",
    "\n",
    "We need two other parameters, the `batch_size` to use for training and the number of nodes to sample at each level of the model. Here we choose a two-level model with 10 nodes sampled in the first layer, and 5 in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_samples = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Neo4JGraphSAGENodeGenerator` object is required to send the node features in sampled subgraphs to Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Neo4JGraphSAGENodeGenerator(G, batch_size, num_samples, neo4j_graphdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `generator.flow()` method, we can create iterators over nodes that should be used to train, validate, or evaluate the model. For training we use only the training nodes returned from our splitter and the target values. The `shuffle=True` argument is given to the `flow` method to improve training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_data.index, train_targets, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify our machine learning model, we need a few more parameters for this:\n",
    "\n",
    " * the `layer_sizes` is a list of hidden feature sizes of each layer in the model. In this example we use 32-dimensional hidden node features at each layer.\n",
    " * The `bias` and `dropout` are internal parameters of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[32, 32], generator=generator, bias=True, dropout=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model to predict the 7 categories using Keras softmax layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp, x_out = graphsage_model.build()\n",
    "prediction = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the actual Keras model with the graph inputs `x_inp` provided by the `graph_model` and outputs being the predictions from the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_inp, outputs=prediction)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.005),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, keeping track of its loss and accuracy on the training set, and its generalisation performance on the test set (we need to create another generator over the test data for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator.flow(test_data.index, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 - 4s - loss: 1.8717 - acc: 0.2889 - val_loss: 1.7331 - val_acc: 0.4212\n",
      "Epoch 2/20\n",
      "6/6 - 3s - loss: 1.6507 - acc: 0.4556 - val_loss: 1.5808 - val_acc: 0.4959\n",
      "Epoch 3/20\n",
      "6/6 - 3s - loss: 1.5011 - acc: 0.6185 - val_loss: 1.4209 - val_acc: 0.6530\n",
      "Epoch 4/20\n",
      "6/6 - 4s - loss: 1.3208 - acc: 0.8148 - val_loss: 1.2929 - val_acc: 0.7256\n",
      "Epoch 5/20\n",
      "6/6 - 3s - loss: 1.2038 - acc: 0.8630 - val_loss: 1.1804 - val_acc: 0.7699\n",
      "Epoch 6/20\n",
      "6/6 - 3s - loss: 1.0483 - acc: 0.9185 - val_loss: 1.0920 - val_acc: 0.7838\n",
      "Epoch 7/20\n",
      "6/6 - 4s - loss: 0.9495 - acc: 0.9259 - val_loss: 1.0150 - val_acc: 0.7994\n",
      "Epoch 8/20\n",
      "6/6 - 3s - loss: 0.8638 - acc: 0.9407 - val_loss: 0.9414 - val_acc: 0.8167\n",
      "Epoch 9/20\n",
      "6/6 - 3s - loss: 0.7612 - acc: 0.9593 - val_loss: 0.8881 - val_acc: 0.8191\n",
      "Epoch 10/20\n",
      "6/6 - 3s - loss: 0.6842 - acc: 0.9704 - val_loss: 0.8424 - val_acc: 0.8240\n",
      "Epoch 11/20\n",
      "6/6 - 3s - loss: 0.6317 - acc: 0.9630 - val_loss: 0.8045 - val_acc: 0.8105\n",
      "Epoch 12/20\n",
      "6/6 - 3s - loss: 0.5641 - acc: 0.9667 - val_loss: 0.7913 - val_acc: 0.8175\n",
      "Epoch 13/20\n",
      "6/6 - 3s - loss: 0.5052 - acc: 0.9815 - val_loss: 0.7656 - val_acc: 0.8113\n",
      "Epoch 14/20\n",
      "6/6 - 4s - loss: 0.4632 - acc: 0.9778 - val_loss: 0.7267 - val_acc: 0.8154\n",
      "Epoch 15/20\n",
      "6/6 - 4s - loss: 0.4171 - acc: 0.9852 - val_loss: 0.7163 - val_acc: 0.8068\n",
      "Epoch 16/20\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(history):\n",
    "    metrics = sorted(history.history.keys())\n",
    "    metrics = metrics[:len(metrics)//2]\n",
    "    for m in metrics:\n",
    "        # summarize history for metric m\n",
    "        plt.plot(history.history[m])\n",
    "        plt.plot(history.history['val_' + m])\n",
    "        plt.title(m)\n",
    "        plt.ylabel(m)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the model we can evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the predictions themselves for all nodes using another node iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = node_data.index\n",
    "all_mapper = generator.flow(all_nodes)\n",
    "all_predictions = model.predict_generator(all_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions will be the output of the softmax layer, so to get final categories we'll use the `inverse_transform` method of our target attribute specifcation to turn these values back to the original categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_predictions = target_encoding.inverse_transform(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(node_predictions, index=all_nodes).idxmax(axis=1)\n",
    "df = pd.DataFrame({\"Predicted\": results, \"True\": node_data[\"subject\"]})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [directed-graphsage-on-cora-example.ipynb](./../../node-classification/graphsage/graphsage-cora-node-classification-example.ipynb) for **node embedding visualization**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
